{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Grammatical error correction - LLMs\n",
        "\n",
        "This notebook contains the steps taken to process the dataset into sentences which were then turned into different prompts used to query the models. Each model was queried through the OpenAI SDK and API and all the results were saved in the form of JSON file in order to keep the sentences in order wich would've been unnecessarily arduous with simple .txt files. Every set of predictions was then evaluated with ROUGE, BLEU and F0.5 metrics."
      ],
      "metadata": {
        "id": "3MDpCB9vWITQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDOQuxuQMGae",
        "outputId": "e96758e2-7289-4ece-8fa3-77e8707be78b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-29 05:49:47--  https://raw.githubusercontent.com/kanekomasahiro/bert-gec/master/scripts/convert_m2_to_parallel.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1599 (1.6K) [text/plain]\n",
            "Saving to: ‘convert_m2_to_parallel.py’\n",
            "\n",
            "convert_m2_to_paral 100%[===================>]   1.56K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-04-29 05:49:47 (23.3 MB/s) - ‘convert_m2_to_parallel.py’ saved [1599/1599]\n",
            "\n",
            "Collecting openai\n",
            "  Downloading openai-1.23.6-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.6/311.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.1)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.23.6\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/kanekomasahiro/bert-gec/master/scripts/convert_m2_to_parallel.py\n",
        "!pip install openai\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata, drive, files\n",
        "\n",
        "!pip install sacrebleu > /dev/null\n",
        "import sacrebleu\n",
        "\n",
        "!pip install torchmetrics > /dev/null\n",
        "from torchmetrics.functional.text.rouge import rouge_score\n",
        "\n",
        "from pprint import pprint\n",
        "import json\n",
        "\n",
        "!pip install errant > /dev/null\n",
        "\n",
        "!python3 -m spacy download en_core_web_sm > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIDRHQ-917Db",
        "outputId": "5da08ad7-c041-4260-b37f-a9f0588db6ec"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.copy(\"/content/drive/MyDrive/INF8225_projet/official-2014.combined.m2\", \"/content/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vdwBfVbCXLNw",
        "outputId": "05f0dbb5-0744-49ec-e14f-4c1574a4cbdd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/official-2014.combined.m2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing\n"
      ],
      "metadata": {
        "id": "MgssJ6HtLGou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python convert_m2_to_parallel.py official-2014.combined.m2 output_src.txt output_tgt.txt"
      ],
      "metadata": {
        "id": "8mdw0U4yYg65"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "\n",
        "with open('/content/output_src.txt', 'r') as f:\n",
        "  for line in f:\n",
        "    sentences.append(line.strip())\n",
        "\n",
        "promptsMinimal = [[{'role': 'system', 'content' : f'Correct the grammatical errors in the following sentence: {sentence}; output:'}] for sentence in sentences]\n",
        "promptsOpen1 = [[{'role': 'system', 'content' : f'Revise mistakes in this text: {sentence}; output:'}] for sentence in sentences]\n",
        "promptsOpen2 = [[{'role': 'system', 'content' : f'Rewrite the following text with proper grammar: {sentence}; output:'}] for sentence in sentences]\n",
        "promptsTool =  [[{'role': 'system', 'content' : f\"\"\"You are a grammatical error correction tool. Your task is to correct the grammaticality and spelling in\n",
        "               the input sentence. Make the smallest possible change in order to make the sentence grammatically\n",
        "               correct. Change as few words as possible. Do not rephrase parts of the sentence that are already\n",
        "               grammatical. Do not change the meaning of the sentence by adding or removing information. If the\n",
        "               sentence is already grammatically correct, you should output the original sentence without changing\n",
        "               anything. \\n\\nInput sentence: {sentence}\\nOutput sentence:\"\"\"}] for sentence in sentences]\n",
        "promptsToolFewShot = [[\n",
        "          {'role': 'system',\n",
        "           'content': \"\"\"You are a grammatical error correction tool. Your task is to correct the grammaticality and spelling in\n",
        "                      the input sentence. Make the smallest possible change in order to make the sentence grammatically\n",
        "                      correct. Change as few words as possible. Do not rephrase parts of the sentence that are already\n",
        "                      grammatical. Do not change the meaning of the sentence by adding or removing information. If the\n",
        "                      sentence is already grammatically correct, you should output the original sentence without changing\n",
        "                      anything.\"\"\"},\n",
        "          {'role': 'user', 'content': 'Input sentence:  I love this sport. I look forward to the weakened, to go out with my bike and my group of friends.'},\n",
        "          {'role': 'assistant', 'content': 'Corrected sentence: I love this sport. I look forward to the weekend to go out with my bike and my group of friends.'},\n",
        "          {'role': 'user', 'content': 'Input sentence:  Lucy Keyes was the last thriller I’ve seen.'},\n",
        "          {'role': 'assistant', 'content': 'Corrected sentence: Lucy Keyes was the last thriller I saw'},\n",
        "          {'role': 'user', 'content': 'Input sentence:  In the biggest cities around the world the traffic nonstop and increase every day.'},\n",
        "          {'role': 'assistant', 'content': 'Corrected sentence: In the biggest cities around the world, the traffic is nonstop and increasing every day.'},\n",
        "          {'role': 'user', 'content': 'Input sentence:  Also, the satisfaction of the customers pushes me to work harder and be better at my job.'},\n",
        "          {'role': 'assistant', 'content': 'Corrected sentence: Also, the satisfaction of the customers pushes me to work harder and be better at my job.'},\n",
        "          {'role': 'user', 'content': f'Input sentence:  {sentence}'}] for sentence in sentences]"
      ],
      "metadata": {
        "id": "pEaaU2gMYpjv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Querying the models"
      ],
      "metadata": {
        "id": "_VJSA8WxS7Zh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clientGPT = OpenAI(api_key=userdata.get('gpt_api_key'))\n",
        "\n",
        "modelsGPT = ['gpt-4-turbo', 'gpt-4', 'gpt-3.5-turbo']\n",
        "\n",
        "anyscale_client = OpenAI(\n",
        "    base_url = \"https://api.endpoints.anyscale.com/v1\",\n",
        "    api_key = userdata.get('anyscale_key')\n",
        ")\n",
        "\n",
        "modelsAnyscale = ['meta-llama/Meta-Llama-3-8B-Instruct', 'mistralai/Mistral-7B-Instruct-v0.1', 'mistralai/Mixtral-8x7B-Instruct-v0.1']\n",
        "\n",
        "models = modelsGPT + modelsAnyscale"
      ],
      "metadata": {
        "id": "IcKkRrNhdAkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputsZeroShot = {model: {} for model in models}\n",
        "outputsFewShot = {model: {} for model in models}\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  for model in modelsGPT:\n",
        "    responseZeroShot = clientGPT.chat.completions.create(\n",
        "        model=model,\n",
        "        messages= promptsMinimal[i],\n",
        "    )\n",
        "    outputsZeroShot[model][i] = responseZeroShot.choices[0].message.content\n",
        "\n",
        "    if model == 'gpt-4-turbo':\n",
        "      responseFewShot = clientGPT.chat.completions.create(\n",
        "          model=model,\n",
        "          messages= promptsToolFewShot[i],\n",
        "      )\n",
        "      outputsFewShot[model][i] = responseFewShot.choices[0].message.content\n",
        "\n",
        "  for model in modelsAnyscale:\n",
        "    responseZeroShot = anyscale_client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages= promptsMinimal[i],\n",
        "    )\n",
        "    outputsZeroShot[model][i] = responseZeroShot.choices[0].message.content\n",
        "\n",
        "    responseFewShot = anyscale_client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages= promptsToolFewShot[i],\n",
        "    )\n",
        "    outputsFewShot[model][i] = responseFewShot.choices[0].message.content\n",
        "\n",
        "  # Save every 100 sentences in case the process stops to avoid losing all\n",
        "  # progress as this takes a few hours\n",
        "  if i % 100 == 0:\n",
        "    print(f'Processed {i} sentences')\n",
        "    for model in modelsGPT:\n",
        "      with open(f'outputsZeroShot_{model}.json', 'w') as f:\n",
        "        json.dump(outputsZeroShot[model], f)\n",
        "      with open(f'outputsFewShot_{model}.json', 'w') as f:\n",
        "        json.dump(outputsFewShot[model], f)\n",
        "    for model in modelsAnyscale:\n",
        "      with open(f'outputsZeroShot_{model.split(\"/\")[-1]}.json', 'w') as f:\n",
        "        json.dump(outputsZeroShot[model], f)\n",
        "      with open(f'outputsFewShot_{model.split(\"/\")[-1]}.json', 'w') as f:\n",
        "        json.dump(outputsFewShot[model], f)"
      ],
      "metadata": {
        "id": "G00PCfvE10R8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model in modelsGPT:\n",
        "  with open(f'outputsZeroShot_{model}.json', 'w') as f:\n",
        "    json.dump(outputsZeroShot[model], f)\n",
        "  shutil.copy(f'/content/outputsZeroShot_{model}.json', '/content/drive/MyDrive/INF8225_projet/')\n",
        "  if model == 'gpt-4-turbo':\n",
        "    with open(f'outputsFewShot_{model}.json', 'w') as f:\n",
        "      json.dump(outputsFewShot[model], f)\n",
        "    shutil.copy(f'/content/outputsFewShot_{model}.json', '/content/drive/MyDrive/INF8225_projet/')\n",
        "for model in modelsAnyscale:\n",
        "  with open(f'outputsZeroShot_{model.split(\"/\")[-1]}.json', 'w') as f:\n",
        "    json.dump(outputsZeroShot[model], f)\n",
        "  shutil.copy(f'/content/outputsZeroShot_{model.split(\"/\")[-1]}.json', '/content/drive/MyDrive/INF8225_projet/')\n",
        "  with open(f'outputsFewShot_{model.split(\"/\")[-1]}.json', 'w') as f:\n",
        "    json.dump(outputsFewShot[model], f)\n",
        "  shutil.copy(f'/content/outputsFewShot_{model.split(\"/\")[-1]}.json', '/content/drive/MyDrive/INF8225_projet/')"
      ],
      "metadata": {
        "id": "pavTzVpcPqfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "p8OP5f8Ae3dH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predsZeroShot = { model: [] for model in models }\n",
        "fewShotModels = modelsAnyscale + ['gpt-4-turbo']\n",
        "predsFewShot = { model: [] for model in fewShotModels }\n",
        "\n",
        "for model in modelsGPT:\n",
        "  with open(f'/content/drive/MyDrive/INF8225_projet/outputsZeroShot_{model}.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "    with open(f'/content/outputsZeroShot_{model}.txt', 'w') as f:\n",
        "      for line in data.values():\n",
        "        predsZeroShot[model].append(line)\n",
        "        f.write(line.replace('\\n', '\\\\n'))\n",
        "        f.write('\\n')\n",
        "  if model == 'gpt-4-turbo':\n",
        "    with open(f'/content/drive/MyDrive/INF8225_projet/outputsFewShot_{model}.json', 'r') as f:\n",
        "      data = json.load(f)\n",
        "      with open(f'/content/outputsFewShot_{model}.txt', 'w') as f:\n",
        "        for line in data.values():\n",
        "          predsFewShot[model].append(line)\n",
        "          f.write(line.replace('\\n', '\\\\n'))\n",
        "          f.write('\\n')\n",
        "\n",
        "for model in modelsAnyscale:\n",
        "  with open(f'/content/drive/MyDrive/INF8225_projet/outputsZeroShot_{model.split(\"/\")[-1]}.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "    with open(f'/content/outputsZeroShot_{model.split(\"/\")[-1]}.txt', 'w') as f:\n",
        "      for line in data.values():\n",
        "        predsZeroShot[model].append(line)\n",
        "        f.write(line.replace('\\n', '\\\\n'))\n",
        "        f.write('\\n')\n",
        "  with open(f'/content/drive/MyDrive/INF8225_projet/outputsFewShot_{model.split(\"/\")[-1]}.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "    with open(f'/content/outputsFewShot_{model.split(\"/\")[-1]}.txt', 'w') as f:\n",
        "      for line in data.values():\n",
        "        predsFewShot[model].append(line)\n",
        "        f.write(line.replace('\\n', '\\\\n'))\n",
        "        f.write('\\n')"
      ],
      "metadata": {
        "id": "O4l4AwggQZqj"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "truths = None\n",
        "with open('/content/drive/MyDrive/INF8225_projet/output_tgt.txt', 'r') as f:\n",
        "  truths = f.readlines()"
      ],
      "metadata": {
        "id": "XWNYodkXUIqa"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model, preds in predsZeroShot.items():\n",
        "  print(f'Model: {model} (Zero-shot)')\n",
        "  print(f'Rouge: {rouge_score(preds, truths)}')\n",
        "  print(f'Bleu: {sacrebleu.corpus_bleu(preds, [truths])}')\n",
        "  print()\n",
        "\n",
        "for model, preds in predsFewShot.items():\n",
        "  print(f'Model: {model} (Few-shot)')\n",
        "  print(f'Rouge: {rouge_score(preds, truths)}')\n",
        "  print(f'Bleu: {sacrebleu.corpus_bleu(preds, [truths])}')\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxxdBSQeUMrC",
        "outputId": "0a37675e-e8e3-4cc7-c33d-1c87a37c5e63"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: gpt-4-turbo (Zero-shot)\n",
            "Rouge: {'rouge1_fmeasure': tensor(0.8940), 'rouge1_precision': tensor(0.8937), 'rouge1_recall': tensor(0.8976), 'rouge2_fmeasure': tensor(0.8015), 'rouge2_precision': tensor(0.8011), 'rouge2_recall': tensor(0.8051), 'rougeL_fmeasure': tensor(0.8885), 'rougeL_precision': tensor(0.8882), 'rougeL_recall': tensor(0.8921), 'rougeLsum_fmeasure': tensor(0.8885), 'rougeLsum_precision': tensor(0.8882), 'rougeLsum_recall': tensor(0.8920)}\n",
            "Bleu: BLEU = 73.71 88.6/78.0/69.4/61.8 (BP = 0.999 ratio = 0.999 hyp_len = 30274 ref_len = 30304)\n",
            "\n",
            "Model: gpt-4 (Zero-shot)\n",
            "Rouge: {'rouge1_fmeasure': tensor(0.8922), 'rouge1_precision': tensor(0.8916), 'rouge1_recall': tensor(0.8961), 'rouge2_fmeasure': tensor(0.7988), 'rouge2_precision': tensor(0.7982), 'rouge2_recall': tensor(0.8026), 'rougeL_fmeasure': tensor(0.8863), 'rougeL_precision': tensor(0.8857), 'rougeL_recall': tensor(0.8902), 'rougeLsum_fmeasure': tensor(0.8863), 'rougeLsum_precision': tensor(0.8857), 'rougeLsum_recall': tensor(0.8901)}\n",
            "Bleu: BLEU = 73.92 88.7/78.3/69.7/62.3 (BP = 0.998 ratio = 0.998 hyp_len = 30235 ref_len = 30304)\n",
            "\n",
            "Model: gpt-3.5-turbo (Zero-shot)\n",
            "Rouge: {'rouge1_fmeasure': tensor(0.8896), 'rouge1_precision': tensor(0.8899), 'rouge1_recall': tensor(0.8926), 'rouge2_fmeasure': tensor(0.7939), 'rouge2_precision': tensor(0.7940), 'rouge2_recall': tensor(0.7968), 'rougeL_fmeasure': tensor(0.8837), 'rougeL_precision': tensor(0.8840), 'rougeL_recall': tensor(0.8867), 'rougeLsum_fmeasure': tensor(0.8837), 'rougeLsum_precision': tensor(0.8839), 'rougeLsum_recall': tensor(0.8867)}\n",
            "Bleu: BLEU = 72.74 88.5/77.5/68.6/60.9 (BP = 0.994 ratio = 0.994 hyp_len = 30136 ref_len = 30304)\n",
            "\n",
            "Model: meta-llama/Meta-Llama-3-8B-Instruct (Zero-shot)\n",
            "Rouge: {'rouge1_fmeasure': tensor(0.5991), 'rouge1_precision': tensor(0.4730), 'rouge1_recall': tensor(0.9016), 'rouge2_fmeasure': tensor(0.5059), 'rouge2_precision': tensor(0.3986), 'rouge2_recall': tensor(0.7691), 'rougeL_fmeasure': tensor(0.5801), 'rougeL_precision': tensor(0.4586), 'rougeL_recall': tensor(0.8708), 'rougeLsum_fmeasure': tensor(0.5892), 'rougeLsum_precision': tensor(0.4653), 'rougeLsum_recall': tensor(0.8860)}\n",
            "Bleu: BLEU = 26.86 35.8/29.0/24.4/20.5 (BP = 1.000 ratio = 2.497 hyp_len = 75662 ref_len = 30304)\n",
            "\n",
            "Model: mistralai/Mistral-7B-Instruct-v0.1 (Zero-shot)\n",
            "Rouge: {'rouge1_fmeasure': tensor(0.8112), 'rouge1_precision': tensor(0.8082), 'rouge1_recall': tensor(0.8334), 'rouge2_fmeasure': tensor(0.6880), 'rouge2_precision': tensor(0.6849), 'rouge2_recall': tensor(0.7077), 'rougeL_fmeasure': tensor(0.8011), 'rougeL_precision': tensor(0.7983), 'rougeL_recall': tensor(0.8222), 'rougeLsum_fmeasure': tensor(0.8019), 'rougeLsum_precision': tensor(0.7989), 'rougeLsum_recall': tensor(0.8237)}\n",
            "Bleu: BLEU = 57.31 75.0/62.0/52.3/44.3 (BP = 1.000 ratio = 1.095 hyp_len = 33178 ref_len = 30304)\n",
            "\n",
            "Model: mistralai/Mixtral-8x7B-Instruct-v0.1 (Zero-shot)\n",
            "Rouge: {'rouge1_fmeasure': tensor(0.5416), 'rouge1_precision': tensor(0.4384), 'rouge1_recall': tensor(0.9028), 'rouge2_fmeasure': tensor(0.4571), 'rouge2_precision': tensor(0.3700), 'rouge2_recall': tensor(0.7706), 'rougeL_fmeasure': tensor(0.5265), 'rougeL_precision': tensor(0.4274), 'rougeL_recall': tensor(0.8722), 'rougeLsum_fmeasure': tensor(0.5341), 'rougeLsum_precision': tensor(0.4325), 'rougeLsum_recall': tensor(0.8891)}\n",
            "Bleu: BLEU = 22.19 29.8/24.1/20.1/16.8 (BP = 1.000 ratio = 2.990 hyp_len = 90606 ref_len = 30304)\n",
            "\n",
            "Model: meta-llama/Meta-Llama-3-8B-Instruct (Few-shot)\n",
            "Rouge: {'rouge1_fmeasure': tensor(0.8619), 'rouge1_precision': tensor(0.8561), 'rouge1_recall': tensor(0.8776), 'rouge2_fmeasure': tensor(0.7549), 'rouge2_precision': tensor(0.7495), 'rouge2_recall': tensor(0.7693), 'rougeL_fmeasure': tensor(0.8543), 'rougeL_precision': tensor(0.8487), 'rougeL_recall': tensor(0.8695), 'rougeLsum_fmeasure': tensor(0.8548), 'rougeLsum_precision': tensor(0.8490), 'rougeLsum_recall': tensor(0.8702)}\n",
            "Bleu: BLEU = 63.98 80.4/68.4/59.2/51.4 (BP = 1.000 ratio = 1.076 hyp_len = 32601 ref_len = 30304)\n",
            "\n",
            "Model: mistralai/Mistral-7B-Instruct-v0.1 (Few-shot)\n",
            "Rouge: {'rouge1_fmeasure': tensor(0.8437), 'rouge1_precision': tensor(0.8481), 'rouge1_recall': tensor(0.8465), 'rouge2_fmeasure': tensor(0.7243), 'rouge2_precision': tensor(0.7278), 'rouge2_recall': tensor(0.7274), 'rougeL_fmeasure': tensor(0.8353), 'rougeL_precision': tensor(0.8396), 'rougeL_recall': tensor(0.8380), 'rougeLsum_fmeasure': tensor(0.8354), 'rougeLsum_precision': tensor(0.8397), 'rougeLsum_recall': tensor(0.8381)}\n",
            "Bleu: BLEU = 64.49 83.4/69.7/59.5/51.0 (BP = 0.995 ratio = 0.995 hyp_len = 30153 ref_len = 30304)\n",
            "\n",
            "Model: mistralai/Mixtral-8x7B-Instruct-v0.1 (Few-shot)\n",
            "Rouge: {'rouge1_fmeasure': tensor(0.8498), 'rouge1_precision': tensor(0.8484), 'rouge1_recall': tensor(0.8628), 'rouge2_fmeasure': tensor(0.7339), 'rouge2_precision': tensor(0.7322), 'rouge2_recall': tensor(0.7467), 'rougeL_fmeasure': tensor(0.8409), 'rougeL_precision': tensor(0.8396), 'rougeL_recall': tensor(0.8535), 'rougeLsum_fmeasure': tensor(0.8412), 'rougeLsum_precision': tensor(0.8398), 'rougeLsum_recall': tensor(0.8541)}\n",
            "Bleu: BLEU = 62.83 80.2/67.5/57.8/49.8 (BP = 1.000 ratio = 1.057 hyp_len = 32040 ref_len = 30304)\n",
            "\n",
            "Model: gpt-4-turbo (Few-shot)\n",
            "Rouge: {'rouge1_fmeasure': tensor(0.8953), 'rouge1_precision': tensor(0.8951), 'rouge1_recall': tensor(0.8986), 'rouge2_fmeasure': tensor(0.8049), 'rouge2_precision': tensor(0.8046), 'rouge2_recall': tensor(0.8082), 'rougeL_fmeasure': tensor(0.8895), 'rougeL_precision': tensor(0.8893), 'rougeL_recall': tensor(0.8928), 'rougeLsum_fmeasure': tensor(0.8895), 'rougeLsum_precision': tensor(0.8893), 'rougeLsum_recall': tensor(0.8928)}\n",
            "Bleu: BLEU = 74.16 88.9/78.5/70.0/62.6 (BP = 0.998 ratio = 0.998 hyp_len = 30235 ref_len = 30304)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!errant_parallel -orig /content/drive/MyDrive/INF8225_projet/output_src.txt -cor /content/drive/MyDrive/INF8225_projet/output_tgt.txt -out truth.m2\n",
        "\n",
        "for model in modelsGPT:\n",
        "  print(f'Model: {model}')\n",
        "  !errant_parallel -orig /content/drive/MyDrive/INF8225_projet/output_src.txt -cor /content/outputsZeroShot_{model}.txt -out preds_{model}.m2\n",
        "  !errant_compare -hyp preds_{model}.m2 -ref truth.m2 -dt\n",
        "  !errant_compare -hyp preds_{model}.m2 -ref truth.m2\n",
        "\n",
        "  if model == 'gpt-4-turbo':\n",
        "    !errant_parallel -orig /content/drive/MyDrive/INF8225_projet/output_src.txt -cor /content/outputsFewShot_{model}.txt -out preds_{model}.m2\n",
        "    !errant_compare -hyp preds_{model}.m2 -ref truth.m2 -dt\n",
        "    !errant_compare -hyp preds_{model}.m2 -ref truth.m2\n",
        "\n",
        "for model in modelsAnyscale:\n",
        "  print(f'Model: {model}')\n",
        "  !errant_parallel -orig /content/drive/MyDrive/INF8225_projet/output_src.txt -cor /content/outputsZeroShot_{model.split(\"/\")[-1]}.txt -out preds_{model.split(\"/\")[-1]}.m2\n",
        "  !errant_compare -hyp preds_{model.split(\"/\")[-1]}.m2 -ref truth.m2 -dt\n",
        "  !errant_compare -hyp preds_{model.split(\"/\")[-1]}.m2 -ref truth.m2\n",
        "\n",
        "  !errant_parallel -orig /content/drive/MyDrive/INF8225_projet/output_src.txt -cor /content/outputsFewShot_{model.split(\"/\")[-1]}.txt -out preds_{model.split(\"/\")[-1]}.m2\n",
        "  !errant_compare -hyp preds_{model.split(\"/\")[-1]}.m2 -ref truth.m2 -dt\n",
        "  !errant_compare -hyp preds_{model.split(\"/\")[-1]}.m2 -ref truth.m2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yAoESIJUZpa",
        "outputId": "62909a28-c9b8-49ed-bb86-e511fc293cfe"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading resources...\n",
            "Processing parallel files...\n",
            "Model: gpt-4-turbo\n",
            "Loading resources...\n",
            "Processing parallel files...\n",
            "\n",
            "=========== Token-Based Detection ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "2606\t5462\t2538\t0.323\t0.5066\t0.3482\n",
            "==============================================\n",
            "\n",
            "\n",
            "=========== Span-Based Correction ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "1171\t4083\t3124\t0.2229\t0.2726\t0.2313\n",
            "==============================================\n",
            "\n",
            "Loading resources...\n",
            "Processing parallel files...\n",
            "\n",
            "=========== Token-Based Detection ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "2560\t5369\t2584\t0.3229\t0.4977\t0.3473\n",
            "==============================================\n",
            "\n",
            "\n",
            "=========== Span-Based Correction ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "1173\t3964\t3122\t0.2283\t0.2731\t0.2361\n",
            "==============================================\n",
            "\n",
            "Model: gpt-4\n",
            "Loading resources...\n",
            "Processing parallel files...\n",
            "\n",
            "=========== Token-Based Detection ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "2520\t5324\t2624\t0.3213\t0.4899\t0.345\n",
            "==============================================\n",
            "\n",
            "\n",
            "=========== Span-Based Correction ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "1135\t3916\t3160\t0.2247\t0.2643\t0.2316\n",
            "==============================================\n",
            "\n",
            "Model: gpt-3.5-turbo\n",
            "Loading resources...\n",
            "Processing parallel files...\n",
            "\n",
            "=========== Token-Based Detection ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "2169\t5115\t2975\t0.2978\t0.4217\t0.3164\n",
            "==============================================\n",
            "\n",
            "\n",
            "=========== Span-Based Correction ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "947\t3598\t3348\t0.2084\t0.2205\t0.2107\n",
            "==============================================\n",
            "\n",
            "Model: meta-llama/Meta-Llama-3-8B-Instruct\n",
            "Loading resources...\n",
            "Processing parallel files...\n",
            "\n",
            "=========== Token-Based Detection ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "2435\t6624\t2709\t0.2688\t0.4734\t0.2942\n",
            "==============================================\n",
            "\n",
            "\n",
            "=========== Span-Based Correction ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "838\t5895\t3457\t0.1245\t0.1951\t0.1342\n",
            "==============================================\n",
            "\n",
            "Loading resources...\n",
            "Processing parallel files...\n",
            "\n",
            "=========== Token-Based Detection ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "2531\t6111\t2613\t0.2929\t0.492\t0.3187\n",
            "==============================================\n",
            "\n",
            "\n",
            "=========== Span-Based Correction ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "978\t4803\t3317\t0.1692\t0.2277\t0.1783\n",
            "==============================================\n",
            "\n",
            "Model: mistralai/Mistral-7B-Instruct-v0.1\n",
            "Loading resources...\n",
            "Processing parallel files...\n",
            "\n",
            "=========== Token-Based Detection ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "2587\t7140\t2557\t0.266\t0.5029\t0.2936\n",
            "==============================================\n",
            "\n",
            "\n",
            "=========== Span-Based Correction ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "859\t5415\t3436\t0.1369\t0.2\t0.1461\n",
            "==============================================\n",
            "\n",
            "Loading resources...\n",
            "Processing parallel files...\n",
            "\n",
            "=========== Token-Based Detection ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "2613\t6670\t2531\t0.2815\t0.508\t0.309\n",
            "==============================================\n",
            "\n",
            "\n",
            "=========== Span-Based Correction ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "905\t5026\t3390\t0.1526\t0.2107\t0.1615\n",
            "==============================================\n",
            "\n",
            "Model: mistralai/Mixtral-8x7B-Instruct-v0.1\n",
            "Loading resources...\n",
            "Processing parallel files...\n",
            "\n",
            "=========== Token-Based Detection ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "2638\t6800\t2506\t0.2795\t0.5128\t0.3075\n",
            "==============================================\n",
            "\n",
            "\n",
            "=========== Span-Based Correction ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "965\t6123\t3330\t0.1361\t0.2247\t0.1478\n",
            "==============================================\n",
            "\n",
            "Loading resources...\n",
            "Processing parallel files...\n",
            "\n",
            "=========== Token-Based Detection ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "2951\t6834\t2193\t0.3016\t0.5737\t0.3332\n",
            "==============================================\n",
            "\n",
            "\n",
            "=========== Span-Based Correction ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "1128\t5500\t3167\t0.1702\t0.2626\t0.1831\n",
            "==============================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}